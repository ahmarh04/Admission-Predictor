{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‘ï¸ Drowsiness Detection using Deep Learning ğŸ§ \n",
    "\n",
    "## ğŸ“š Table of Contents\n",
    "- [Project Overview](#project-overview)\n",
    "- [Dataset Description](#dataset-description)\n",
    "- [Implementation Details](#implementation-details)\n",
    "  - [Data Preprocessing](#data-preprocessing)\n",
    "  - [Model Architecture](#model-architecture)\n",
    "  - [Training and Evaluation](#training-and-evaluation)\n",
    "  - [Results Visualization](#results-visualization)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Project Overview\n",
    "Drowsy driving is a major cause of road accidents, making **drowsiness detection** a crucial real-world application.  \n",
    "This project leverages **Deep Learning (CNN + VGG16)** to classify **whether a person's eyes are open or closed**, providing a foundation for real-time alert systems.\n",
    "\n",
    "**ğŸ”¹ Key Highlights:**\n",
    "- ğŸ–¼ï¸ **Image-based classification**: Detects eye openness from images.\n",
    "- âš¡ **Hybrid Model**: Combines **Custom CNN** & **Pretrained VGG16** for robust feature extraction.\n",
    "- ğŸ¯ **Real-time applicability**: Can be extended for real-time detection in embedded systems.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Dataset Description\n",
    "The **Drowsiness Detection Dataset** used which is a curated combination of multiple open-source datasets:\n",
    "\n",
    "1. **MRL Dataset**\n",
    "2. **Closed Eyes in the Wild (CEW) Dataset**\n",
    "3. **Custom Dataset** (additional images for better generalization)\n",
    "\n",
    "### ğŸ”¹ **Key Features:**\n",
    "- ğŸ–¼ï¸ **Diverse Capture Conditions**:\n",
    "  - ğŸ“¸ Different lighting setups\n",
    "  - ğŸ“ Varying distances & resolutions\n",
    "  - ğŸ”„ Various face & eye angles  \n",
    "- âš–ï¸ **Balanced Class Distribution** (Open Eyes vs. Closed Eyes)\n",
    "- ğŸ“¦ **Dataset Variants**:\n",
    "  - **V1:** 10,000 images\n",
    "  - **V2:** 5,000 images\n",
    "  - **V3:** 10,000 images\n",
    "  - **V4:** 4,000 images\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Implementation Details\n",
    "### ğŸ—ï¸ **1. Data Preprocessing**\n",
    "- Image resizing to **224x224 pixels** ğŸ“  \n",
    "- Data augmentation for improved generalization âœ¨  \n",
    "- Train-test split (**80% training, 20% testing**) ğŸ“Š  \n",
    "\n",
    "### ğŸ›ï¸ **2. Model Architecture**\n",
    "- **Custom CNN Model:** Feature extraction from images  \n",
    "- **Pretrained VGG16:** Transfer learning for better accuracy  \n",
    "- **Fully Connected Layers:** Combines extracted features for final classification  \n",
    "\n",
    "### ğŸ¯ **3. Training and Evaluation**\n",
    "- **Optimizer:** Adam (Adaptive Learning Rate) âš¡  \n",
    "- **Loss Function:** Categorical Crossentropy ğŸ¯  \n",
    "- **Metric:** Accuracy (%) ğŸ“ˆ  \n",
    "- **Best Model Checkpointing** for saving optimal performance weights ğŸ’¾  \n",
    "\n",
    "### ğŸ“Š **4. Results Visualization**\n",
    "- **Confusion Matrix** for classification analysis ğŸ”  \n",
    "- **Training Curves (Loss & Accuracy)** ğŸ“‰ğŸ“ˆ  \n",
    "- **Misclassified Images Display** for debugging ğŸ§  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸ”¥ **Next Step:** Proceed with **Cell-1: Importing Libraries & Setup** ğŸš€  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Œ **Step 1: Importing Required Libraries & Defining Paths**\n",
    "# ---------------------------------------------------------------\n",
    "### This cell imports the necessary libraries required for:\n",
    "- âœ… Deep Learning: TensorFlow & Keras\n",
    "- âœ… Data Handling: NumPy & OS\n",
    "- âœ… Dataset Splitting: Sklearn (Train-Test Split)\n",
    "### It also defines the dataset directory and standard parameters like:\n",
    "- ğŸ–¼ï¸ Image dimensions (224x224 pixels)\n",
    "- ğŸ“¦ Batch size for training (32)\n",
    "### ğŸ”¹ The dataset directory should be updated to the correct path before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T16:35:16.087256Z",
     "iopub.status.busy": "2025-02-17T16:35:16.086959Z",
     "iopub.status.idle": "2025-02-17T16:35:27.891808Z",
     "shell.execute_reply": "2025-02-17T16:35:27.891149Z",
     "shell.execute_reply.started": "2025-02-17T16:35:16.087228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess dataset\n",
    "dataset_dir = \"/kaggle/input/mrl-dataset/train\"  # Replace with the path to your dataset\n",
    "img_height, img_width = 224, 224  \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š **Step 2: Loading & Preprocessing the Dataset**\n",
    "\n",
    "## In this cell, we:\n",
    "\n",
    "### ğŸ”¹ **Use ImageDataGenerator for Data Augmentation:**  \n",
    "- This allows us to rescale pixel values to the range `[0, 1]`, which accelerates model convergence.  \n",
    "- No additional augmentations (like rotation or flipping) are applied to keep the dataset consistent.  \n",
    "\n",
    "### ğŸ”¹ **Load Images and Labels:**  \n",
    "- `flow_from_directory()` loads images directly from the directory structure.  \n",
    "- Subfolder names are automatically treated as class labels.  \n",
    "- `shuffle=False` ensures that the order of images is preserved, useful for consistent train-test splits.  \n",
    "\n",
    "### ğŸ”¹ **Convert Data to NumPy Arrays:**  \n",
    "- Images (`x_data`) and labels (`y_data`) are stored in lists and then concatenated into NumPy arrays for faster computation.  \n",
    "\n",
    "### ğŸ”¹ **Split Data into Training and Testing Sets:**  \n",
    "- `train_test_split()` is used to split the data into **80% training** and **20% testing**.  \n",
    "- The `random_state` is fixed for reproducibility.  \n",
    "\n",
    "### ğŸ“Œ **Why This Step Matters:**  \n",
    "Proper data preprocessing and consistent splitting are essential for building a reliable and generalizable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T16:35:27.893201Z",
     "iopub.status.busy": "2025-02-17T16:35:27.892725Z",
     "iopub.status.idle": "2025-02-17T16:35:47.851240Z",
     "shell.execute_reply": "2025-02-17T16:35:47.850451Z",
     "shell.execute_reply.started": "2025-02-17T16:35:27.893149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define ImageDataGenerator for loading images and labels\n",
    "datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Load the images from the directory without splitting\n",
    "data_flow = datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False  # Make sure data is not shuffled for consistent splits\n",
    ")\n",
    "\n",
    "# Store images (x) and labels (y)\n",
    "x_data, y_data = [], []\n",
    "\n",
    "# Load all the data\n",
    "for i in range(len(data_flow)):\n",
    "    x_batch, y_batch = data_flow[i]\n",
    "    x_data.append(x_batch)\n",
    "    y_data.append(y_batch)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "x_data = np.concatenate(x_data, axis=0)\n",
    "y_data = np.concatenate(y_data, axis=0)\n",
    "\n",
    "# Split data into train and test (80-20 split)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  **Step 3: Building a Hybrid Model (Simple CNN + VGG16)**\n",
    "# ---------------------------------------------------------------\n",
    "### This cell builds a deep learning model that combines:\n",
    "- âœ… A **Custom CNN Model** (Extracts features from raw images)\n",
    "- âœ… A **Pretrained VGG16 Model** (Extracts additional feature representations)\n",
    "- âœ… A **Fully Connected Layer** (Combines features for classification)\n",
    "\n",
    "### ğŸ”¹ The Simple CNN has:\n",
    "-    ğŸ”¸ Multiple Conv2D layers with different activation functions (ReLU, Softplus, Tanh, ELU)\n",
    "-    ğŸ”¸ MaxPooling for feature downsampling\n",
    "-    ğŸ”¸ Global Average Pooling for feature reduction\n",
    "\n",
    "### ğŸ”¹ VGG16:\n",
    "-    ğŸ”¸ Pretrained on ImageNet dataset\n",
    "-    ğŸ”¸ Frozen layers (not trainable) to preserve learned features\n",
    "\n",
    "### ğŸ”¹ The final model concatenates both networks and adds:\n",
    "-    ğŸ”¸ A Dense layer (256 neurons, ReLU activation)\n",
    "-    ğŸ”¸ A Dropout layer (50% dropout to prevent overfitting)\n",
    "-    ğŸ”¸ An Output layer with softmax activation (for multi-class classification)\n",
    "\n",
    "ğŸ”¹ **Why This Architecture?**  \n",
    "- Combining custom layers with a pre-trained model harnesses the power of transfer learning while maintaining flexibility for domain-specific features.  \n",
    "- This hybrid approach is especially powerful when dealing with limited data or complex image categories.  \n",
    "\n",
    "ğŸ”” **Tip:** This model architecture is flexibleâ€”feel free to experiment with different activation functions or layer configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T16:35:47.853338Z",
     "iopub.status.busy": "2025-02-17T16:35:47.853109Z",
     "iopub.status.idle": "2025-02-17T16:35:50.740930Z",
     "shell.execute_reply": "2025-02-17T16:35:50.740146Z",
     "shell.execute_reply.started": "2025-02-17T16:35:47.853319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models, Input, Model\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (img_height, img_width, 3)\n",
    "\n",
    "# Simple CNN (Model 1)\n",
    "def create_simple_cnn(input_shape):\n",
    "    cnn_input = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation=\"relu\")(cnn_input)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation=\"softplus\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation=\"tanh\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(256, (3, 3), activation=\"elu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=cnn_input, outputs=x, name=\"SimpleCNN\")\n",
    "\n",
    "simple_cnn = create_simple_cnn(input_shape)\n",
    "\n",
    "# VGG16 (Model 2) for feature extraction\n",
    "vgg16_base = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "vgg16_base.trainable = False  # Freeze VGG16 layers\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Extract features from Simple CNN\n",
    "simple_cnn_features = simple_cnn(input_layer)\n",
    "\n",
    "# Extract features from VGG16\n",
    "vgg16_features = vgg16_base(input_layer)\n",
    "vgg16_features = layers.GlobalAveragePooling2D()(vgg16_features)\n",
    "\n",
    "# Concatenate features\n",
    "concatenated_features = layers.concatenate([simple_cnn_features, vgg16_features])\n",
    "\n",
    "# Add fully connected layers for classification\n",
    "x = layers.Dense(256, activation=\"relu\")(concatenated_features)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output_layer = layers.Dense(data_flow.num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Final model\n",
    "combined_model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš™ï¸ Step 4: Compiling the Combined Model\n",
    "\n",
    "## Here we:\n",
    "\n",
    "### ğŸ”¹ **Choose the Optimizer:**  \n",
    "- `Adam` optimizer is used for efficient training with adaptive learning rates.  \n",
    "- Adam is popular for its robustness and fast convergence.  \n",
    "\n",
    "### ğŸ”¹ **Specify the Loss Function:**  \n",
    "- `Categorical Crossentropy` is ideal for multi-class classification tasks.  \n",
    "- It measures the divergence between the predicted and true class probabilities.  \n",
    "\n",
    "### ğŸ”¹ **Set Evaluation Metrics:**  \n",
    "- `Accuracy` is tracked as the primary performance metric.  \n",
    "\n",
    "### ğŸ”¹ **Summary of the Model:**  \n",
    "- `summary()` provides a layer-by-layer overview of the model architecture, including:  \n",
    "  - Layer names and types.  \n",
    "  - Output shapes.  \n",
    "  - Number of trainable and non-trainable parameters.  \n",
    "\n",
    "### ğŸ“Œ **Why Compilation Matters:**  \n",
    "Compiling defines how the model learns (optimizer), how the loss is calculated (loss function), and what metrics to monitor during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T16:35:50.742271Z",
     "iopub.status.busy": "2025-02-17T16:35:50.741989Z",
     "iopub.status.idle": "2025-02-17T16:35:50.770115Z",
     "shell.execute_reply": "2025-02-17T16:35:50.769507Z",
     "shell.execute_reply.started": "2025-02-17T16:35:50.742240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "combined_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Summary of the model\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ–¼ï¸ **Step 5: Visualizing the Model Architecture**\n",
    "# ---------------------------------------------------------------\n",
    "- âœ… Uses `plot_model()` to generate a visual representation of the model.\n",
    "- âœ… Displays:\\\n",
    "      ğŸ”¹ Layer Names\\\n",
    "      ğŸ”¹ Input & Output Shapes\\\n",
    "      ğŸ”¹ Connections between different layers\n",
    "\n",
    " ğŸ“Œ **Helps in understanding the structure of the deep learning model before training.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T16:35:50.771114Z",
     "iopub.status.busy": "2025-02-17T16:35:50.770855Z",
     "iopub.status.idle": "2025-02-17T16:35:51.310855Z",
     "shell.execute_reply": "2025-02-17T16:35:51.309943Z",
     "shell.execute_reply.started": "2025-02-17T16:35:50.771085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Plot the model architecture\n",
    "plot_model(combined_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Step 6: Training the Model with Checkpoints\n",
    "\n",
    "In this step:\n",
    "\n",
    "ğŸ”¹ **Define a ModelCheckpoint Callback:**  \n",
    "- Monitors `val_accuracy` to save only the best-performing model.  \n",
    "- `save_best_only=True` prevents overwriting the best model during training.  \n",
    "\n",
    "ğŸ”¹ **Train the Model:**  \n",
    "- The model is trained for `50 epochs` using the training data (`x_train` and `y_train`).  \n",
    "- `validation_data` is used to monitor generalization performance.  \n",
    "- Training time is recorded for performance analysis.  \n",
    "\n",
    "ğŸ”¹ **Why Use Callbacks?**  \n",
    "- To **save time and resources** by storing only the most accurate model.  \n",
    "- To **resume training** from checkpoints if interrupted.  \n",
    "\n",
    "ğŸ§ª **Experimentation:** Try adjusting the learning rate, batch size, or number of epochs to improve accuracy and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T16:35:51.311829Z",
     "iopub.status.busy": "2025-02-17T16:35:51.311618Z",
     "iopub.status.idle": "2025-02-17T16:56:09.930238Z",
     "shell.execute_reply": "2025-02-17T16:56:09.929487Z",
     "shell.execute_reply.started": "2025-02-17T16:35:51.311811Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import time\n",
    "\n",
    "# Define the checkpoint callback\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"best_combined_model3.keras\",  # Filepath to save the model\n",
    "    monitor=\"val_accuracy\",  # Monitor validation loss\n",
    "    save_best_only=True,  # Save only the best model\n",
    "    mode=\"max\",  # Save the model with the minimum validation loss\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model with the callback\n",
    "epochs = 50\n",
    "history = combined_model.fit(\n",
    "    x_train,y_train,\n",
    "    validation_data=(x_test,y_test),\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint]  # Add the checkpoint callback\n",
    ")\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â²ï¸ Step 7: Analyzing Training Duration\n",
    "\n",
    "Here we:\n",
    "\n",
    "ğŸ”¹ **Calculate Total Training Time:**  \n",
    "- Display the time taken to train the model using `time.time()`.\n",
    "\n",
    "ğŸ”¹ **Why Record Training Time?**  \n",
    "- Useful for benchmarking model performance.  \n",
    "- Helps in optimizing resource allocation for future experiments.  \n",
    "\n",
    "â±ï¸ **Insight:** If training time is too long, consider optimizing the model by reducing the number of layers, using smaller images, or using a more efficient optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T16:56:09.931507Z",
     "iopub.status.busy": "2025-02-17T16:56:09.931172Z",
     "iopub.status.idle": "2025-02-17T16:56:09.936118Z",
     "shell.execute_reply": "2025-02-17T16:56:09.935264Z",
     "shell.execute_reply.started": "2025-02-17T16:56:09.931477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Total training time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ **Step 8: Evaluating the Model on Train & Test Data**\n",
    "# ---------------------------------------------------------------\n",
    "- âœ… Load the **best saved model**.\n",
    "- âœ… Evaluates it on:\\\n",
    "    ğŸ”¹ Training Dataset\\\n",
    "    ğŸ”¹ Testing Dataset\n",
    "- âœ… Prints:\\\n",
    "    ğŸ”¹ Train & Test Loss\\\n",
    "    ğŸ”¹ Train & Test Accuracy\n",
    "\n",
    "### ğŸ“Œ **This helps in understanding model performance and overfitting tendencies.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T16:56:09.937432Z",
     "iopub.status.busy": "2025-02-17T16:56:09.937031Z",
     "iopub.status.idle": "2025-02-17T16:56:37.610293Z",
     "shell.execute_reply": "2025-02-17T16:56:37.609472Z",
     "shell.execute_reply.started": "2025-02-17T16:56:09.937317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the best model\n",
    "best_model = load_model(\"best_combined_model3.keras\")\n",
    "\n",
    "# Evaluate on the training and test sets\n",
    "train_loss, train_accuracy = best_model.evaluate(x_train, y_train)\n",
    "test_loss, test_accuracy = best_model.evaluate(x_test, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” **Step 9: Analyzing Model Performance using Confusion Matrix**\n",
    "# ---------------------------------------------------------------\n",
    "- âœ… Generates predictions on the test dataset.\n",
    "- âœ… Computes a **Confusion Matrix** to visualize:\\\n",
    "    ğŸ”¹ Correct & Incorrect Predictions\\\n",
    "    ğŸ”¹ Class-wise accuracy\n",
    "- âœ… Uses **Seaborn Heatmap** for better visualization.\n",
    "- âœ… Computes:\\\n",
    "    ğŸ”¹ Precision, Recall, and F1-Score\n",
    "\n",
    "### ğŸ“Œ **Confusion Matrix helps in identifying misclassified samples.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T16:56:50.004868Z",
     "iopub.status.busy": "2025-02-17T16:56:50.004571Z",
     "iopub.status.idle": "2025-02-17T16:56:56.489339Z",
     "shell.execute_reply": "2025-02-17T16:56:56.488709Z",
     "shell.execute_reply.started": "2025-02-17T16:56:50.004842Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Generate Predictions\n",
    "y_pred = best_model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n",
    "\n",
    "# Step 2: Get True Labels (y_test is one-hot encoded, so we convert it to class labels)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Step 3: Compute Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Define class labels (for drowsiness detection: 0 = Closed Eyes, 1 = Open Eyes)\n",
    "class_labels = ['Closed Eyes', 'Open Eyes']\n",
    "\n",
    "# Step 4: Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Compute Classification Report\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=class_labels))\n",
    "\n",
    "# Optionally, compute Precision, Recall, and F1-Score individually\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_true, y_pred, average='binary')\n",
    "recall = recall_score(y_true, y_pred, average='binary')\n",
    "f1 = f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“‰ **Step 10: Evaluating Model Performance with ROC-AUC Curve**\n",
    "# ---------------------------------------------------------------\n",
    "- âœ… Computes **ROC-AUC Score** to measure classification effectiveness.\n",
    "- âœ… Plots **True Positive Rate (TPR) vs. False Positive Rate (FPR)**.\n",
    "- âœ… Helps in understanding the model's ability to separate different classes.\n",
    "\n",
    "### ğŸ“Œ **A higher AUC score indicates better classification performance.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T16:59:11.827134Z",
     "iopub.status.busy": "2025-02-17T16:59:11.826835Z",
     "iopub.status.idle": "2025-02-17T16:59:16.505574Z",
     "shell.execute_reply": "2025-02-17T16:59:16.504331Z",
     "shell.execute_reply.started": "2025-02-17T16:59:11.827111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Ensure y_test is in label format (not one-hot encoded)\n",
    "if y_test.ndim == 2:\n",
    "    y_true = np.argmax(y_test, axis=1)  # Convert one-hot labels to single-column labels\n",
    "else:\n",
    "    y_true = y_test  # Already in correct format\n",
    "\n",
    "# Ensure y_pred contains probabilities, not class labels\n",
    "if hasattr(best_model, \"predict_proba\"):  # Check if model supports probability predictions\n",
    "    y_pred_prob = best_model.predict_proba(x_test)[:, 1]  # Use probability for the positive class (Open Eyes)\n",
    "else:\n",
    "    print(\"Warning: Model does not support predict_proba. Using sigmoid/logits if applicable.\")\n",
    "    y_pred_prob = best_model.predict(x_test)  # Assuming this gives probabilities (e.g., for sigmoid activation)\n",
    "    y_pred_prob = y_pred_prob.flatten()  # Ensure it's 1D\n",
    "\n",
    "# Now y_pred_prob should have the correct shape (800,)\n",
    "\n",
    "# Compute ROC-AUC Score\n",
    "roc_auc = roc_auc_score(y_true, y_pred_prob)\n",
    "\n",
    "# Compute ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "\n",
    "# Print ROC-AUC Score\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC: {roc_auc:.2f})', color='blue', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')  # Diagonal reference line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (Drowsiness Detection)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¥ **Step 11: Identifying & Displaying Misclassified Samples**\n",
    "# ---------------------------------------------------------------\n",
    "- âœ… Extracts misclassified images where:\\\n",
    "-    ğŸ”¹ True label â‰  Predicted label\n",
    "- âœ… Displays up to **16 misclassified images**.\n",
    "- âœ… Helps in understanding **failure cases & potential biases** in the model.\n",
    "\n",
    "### ğŸ“Œ **Useful for debugging and improving model generalization!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T19:57:40.697965Z",
     "iopub.status.busy": "2025-02-16T19:57:40.697646Z",
     "iopub.status.idle": "2025-02-16T19:57:42.514509Z",
     "shell.execute_reply": "2025-02-16T19:57:42.513553Z",
     "shell.execute_reply.started": "2025-02-16T19:57:40.697940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define class labels (Modify based on your dataset)\n",
    "class_labels = [\"Closed\", \"Open\"]  # Assuming 0 = Closed, 1 = Open\n",
    "\n",
    "# Find misclassified indices\n",
    "misclassified_indices = np.where(y_true != y_pred)[0]\n",
    "\n",
    "# Limit the number of images displayed to avoid overcrowding\n",
    "num_images = min(len(misclassified_indices), 16)  # Show up to 16 misclassified images\n",
    "misclassified_indices = misclassified_indices[:num_images]\n",
    "\n",
    "# Set up grid for visualization\n",
    "rows = int(np.ceil(num_images / 4))  # Dynamically adjust rows\n",
    "fig, axes = plt.subplots(rows, 4, figsize=(10, 2.5 * rows))  \n",
    "axes = axes.ravel()  # Flatten the 2D array for easy indexing\n",
    "\n",
    "for i, idx in enumerate(misclassified_indices):\n",
    "    axes[i].imshow(x_test[idx], cmap=\"gray\")  # Use grayscale if needed\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(f\"True: {class_labels[y_true[idx]]}\\nPred: {class_labels[y_pred[idx]]}\")\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(num_images, len(axes)):\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“‰ **Step 12: Visualizing Training & Validation Loss**\n",
    "# ---------------------------------------------------------------\n",
    "- âœ… Plots the **Loss Curve** for:\\\n",
    "    ğŸ”¹ Training Dataset\\\n",
    "    ğŸ”¹ Validation Dataset\n",
    "- âœ… Helps in detecting:\\\n",
    "     ğŸ”¹ Overfitting (if validation loss increases)\\\n",
    "     ğŸ”¹ Underfitting (if both losses remain high)\n",
    "\n",
    "### ğŸ“Œ **A well-trained model should have decreasing loss curves over epochs.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T19:57:59.870758Z",
     "iopub.status.busy": "2025-02-16T19:57:59.870445Z",
     "iopub.status.idle": "2025-02-16T19:58:00.132284Z",
     "shell.execute_reply": "2025-02-16T19:58:00.131518Z",
     "shell.execute_reply.started": "2025-02-16T19:57:59.870733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'history' is the result of model.fit()\n",
    "history_dict = history.history\n",
    "\n",
    "# Extract data\n",
    "epochs = range(1, len(history_dict['loss']) + 1)\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "train_accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, 'bo-', label='Training Loss')  # 'bo-' for blue dots\n",
    "plt.plot(epochs, val_loss, 'r*-', label='Validation Loss')  # 'r*-' for red stars\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ **Step 13: Visualizing Training & Validation Accuracy**\n",
    "# ---------------------------------------------------------------\n",
    "- âœ… Plots the **Accuracy Curve** for:\\\n",
    "    ğŸ”¹ Training Dataset\\\n",
    "    ğŸ”¹ Validation Dataset\n",
    "- âœ… Ensures that the model is **learning & generalizing well**.\n",
    "\n",
    "### ğŸ“Œ **A good model should show increasing accuracy over epochs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T19:58:03.685928Z",
     "iopub.status.busy": "2025-02-16T19:58:03.685567Z",
     "iopub.status.idle": "2025-02-16T19:58:03.903726Z",
     "shell.execute_reply": "2025-02-16T19:58:03.902846Z",
     "shell.execute_reply.started": "2025-02-16T19:58:03.685882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracy, 'bo-', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'r*-', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1048759,
     "sourceId": 1770390,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
